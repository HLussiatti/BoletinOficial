{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3259cf",
   "metadata": {},
   "source": [
    "# Imports & Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "422d7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from functions import post, parse, nuevos_avisos, guardar_avisos, descargar_pdf\n",
    "#from NLP import extraer_texto_pdf, resumir_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3d16fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_clave = {'energía','energético'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dc8831b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar la base de datos existente\n",
    "database = pd.read_excel('database.xlsx')\n",
    "database = database.drop(index=database.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bf906",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fd3ad59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar logger\n",
    "logging.basicConfig(\n",
    "    filename='boletin_errors.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "84382cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avisos_totales = []\n",
    "for palabra in palabras_clave :\n",
    "    try:\n",
    "        response = post(palabra)\n",
    "        if response.status_code == 200:\n",
    "            datos = response.json()\n",
    "            avisos_totales.extend(parse(datos))\n",
    "        else:\n",
    "            logging.warning(f\"Error al buscar '{palabra}': Código {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fallo la búsqueda para '{palabra}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b5a24c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if avisos_totales is not None:\n",
    "    df_nuevos_avisos = nuevos_avisos(avisos_totales, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b6fbcc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PyPDF2 import PdfReader\n",
    "# from transformers import pipeline\n",
    "\n",
    "# def resumir_pdf(path_pdf):\n",
    "#     # Leer texto del PDF\n",
    "#     reader = PdfReader(path_pdf)\n",
    "#     texto = \"\"\n",
    "#     for page in reader.pages:\n",
    "#         texto += page.extract_text()\n",
    "    \n",
    "#     # Inicializar modelo de resumen\n",
    "#     summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#     # Dividir texto en partes si es necesario\n",
    "#     max_chunk = 1024\n",
    "#     chunks = [texto[i:i+max_chunk] for i in range(0, len(texto), max_chunk)]\n",
    "#     print (chunks)\n",
    "#     resumen = \"\"\n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         resumen += summarizer(chunk, max_length=250, min_length=100, do_sample=False)[0]['summary_text'] + \"\\n\"\n",
    "\n",
    "#     return resumen.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e27a9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "# def dividir_en_chunks(texto, max_palabras=200):\n",
    "#     palabras = texto.split()\n",
    "#     return [\" \".join(palabras[i:i + max_palabras]) for i in range(0, len(palabras), max_palabras)]\n",
    "\n",
    "# def resumir_con_bart(texto, hf_token):\n",
    "#     if len(texto.strip()) < 50:\n",
    "#         raise ValueError(\"Chunk demasiado corto para resumir.\")\n",
    "    \n",
    "#     api_url = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
    "    \n",
    "#     headers = {\"Authorization\": f\"Bearer {\"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\"}\"}\n",
    "#     payload = {\"inputs\": texto}\n",
    "#     response = requests.post(api_url, headers=headers, json=payload)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()[0]['summary_text']\n",
    "#     else:\n",
    "#         raise Exception(f\"[Resumen] Error {response.status_code}: {response.text}\")\n",
    "\n",
    "# def traducir_al_espanol(texto, hf_token):\n",
    "#     url = \"https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-es\"\n",
    "#     headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "#     payload = {\"inputs\": texto}\n",
    "#     response = requests.post(url, headers=headers, json=payload)\n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()[0]['translation_text']\n",
    "#     else:\n",
    "#         raise Exception(f\"[Traducción] Error {response.status_code}: {response.text}\")\n",
    "\n",
    "# def resumen_partes_pdf_traducido(path_pdf, hf_token):\n",
    "#     # Leer PDF\n",
    "#     reader = PdfReader(path_pdf)\n",
    "#     texto = \"\"\n",
    "#     for page in reader.pages:\n",
    "#         page_text = page.extract_text()\n",
    "#         if page_text:\n",
    "#             texto += page_text + \"\\n\"\n",
    "\n",
    "#     texto = texto.upper()\n",
    "#     inicio_considerando = texto.find(\"CONSIDERANDO\")\n",
    "#     inicio_resuelve = texto.find(\"RESUELVE\")\n",
    "\n",
    "#     if inicio_considerando == -1 or inicio_resuelve == -1:\n",
    "#         raise ValueError(\"No se encontraron las secciones CONSIDERANDO y/o RESUELVE.\")\n",
    "\n",
    "#     considerandos = texto[inicio_considerando:inicio_resuelve].strip()\n",
    "#     resuelve = texto[inicio_resuelve:].strip()\n",
    "\n",
    "#     partes_considerando = [chunk for chunk in dividir_en_chunks(considerandos) if len(chunk.strip()) >= 50]\n",
    "#     partes_resuelve = [chunk for chunk in dividir_en_chunks(resuelve) if len(chunk.strip()) >= 50]\n",
    "\n",
    "\n",
    "#     resumen_considerando = \"\\n\".join([\n",
    "#         traducir_al_espanol(resumir_con_bart(chunk, hf_token), hf_token)\n",
    "#         for chunk in partes_considerando\n",
    "#         if len(chunk.strip().split()) >= 30\n",
    "#     ])\n",
    "#     resumen_resuelve = \"\\n\".join([\n",
    "#         traducir_al_espanol(resumir_con_bart(chunk, hf_token), hf_token)\n",
    "#         for chunk in partes_resuelve\n",
    "#     ])\n",
    "\n",
    "#     return {\n",
    "#         \"considerandos\": resumen_considerando.strip(),\n",
    "#         \"resuelve\": resumen_resuelve.strip()\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c706f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "\n",
    "def extraer_texto_pdf(path_pdf, guardar_txt=None, margen_sup=80, margen_inf=80):\n",
    "    \"\"\"\n",
    "    Extrae texto de un PDF oficial, separando:\n",
    "    - CONSIDERANDO: por punto y salto de línea.\n",
    "    - RESUELVE: por cada ARTÍCULO.\n",
    "    \"\"\"\n",
    "\n",
    "    texto_crudo = \"\"\n",
    "\n",
    "    # Extraer texto recortando encabezado y pie\n",
    "    with pdfplumber.open(path_pdf) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            altura = page.height\n",
    "            area_util = page.within_bbox((0, margen_sup, page.width, altura - margen_inf))\n",
    "            texto_pagina = area_util.extract_text()\n",
    "            if texto_pagina:\n",
    "                texto_crudo += texto_pagina + \"\\n\"\n",
    "\n",
    "    # Normalizar espacios y saltos\n",
    "    texto_crudo = re.sub(r'[ \\t]+', ' ', texto_crudo)\n",
    "    texto_crudo = re.sub(r'\\n+', '\\n', texto_crudo).strip()\n",
    "\n",
    "    # Buscar las secciones CONSIDERANDO y RESUELVE\n",
    "    match_considerando = re.search(r'CONSIDERANDO:', texto_crudo, re.IGNORECASE)\n",
    "    match_resuelve = re.search(r'RESUELVE:', texto_crudo, re.IGNORECASE)\n",
    "\n",
    "    if not match_considerando or not match_resuelve:\n",
    "        raise ValueError(\"No se encontraron ambas secciones 'CONSIDERANDO' y 'RESUELVE'.\")\n",
    "\n",
    "    # Dividir texto en tres partes: preconsiderandos (opcional), considerando, resuelve\n",
    "    texto_considerando = texto_crudo[match_considerando.end():match_resuelve.start()].strip()\n",
    "    texto_resuelve = texto_crudo[match_resuelve.end():].strip()\n",
    "\n",
    "    # === Procesar CONSIDERANDO ===\n",
    "    # Unir líneas, cortar por punto seguido de salto de línea\n",
    "    texto_considerando = re.sub(r'\\n', ' ', texto_considerando)\n",
    "    considerando_parrafos = re.split(r'\\.\\s+', texto_considerando)\n",
    "    considerando_parrafos = [p.strip() + '.' for p in considerando_parrafos if p.strip()]\n",
    "\n",
    "    # === Procesar RESUELVE ===\n",
    "    # Separar por artículos\n",
    "    articulos = re.split(r'\\b(ART[IÍ]CULO\\s+\\d+°?)', texto_resuelve)\n",
    "    articulos_limpios = []\n",
    "    for i in range(1, len(articulos), 2):  # saltamos de a 2: etiqueta y contenido\n",
    "        etiqueta = articulos[i].strip()\n",
    "        contenido = articulos[i + 1].strip().replace('\\n', ' ')\n",
    "        articulos_limpios.append(f\"{etiqueta} {contenido}\")\n",
    "\n",
    "    # Consolidar texto final\n",
    "    texto_final = \"CONSIDERANDO:\\n\\n\" + \"\\n\\n\".join(considerando_parrafos)\n",
    "    texto_final += \"\\n\\n\\nRESUELVE:\\n\\n\" + \"\\n\\n\".join(articulos_limpios)\n",
    "\n",
    "    # Guardar en .txt si se solicita\n",
    "    if guardar_txt:\n",
    "        with open(guardar_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(texto_final)\n",
    "\n",
    "    return texto_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "7f95bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def resumir_con_gpt4(texto, seccion, modelo=\"gpt-3.5-turbo\"):\n",
    "\n",
    "    API_KEY = \"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\"\n",
    "    CHATGPT_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Este es el contenido de la sección '{seccion}' de una resolución oficial argentina:\\n\\n\"\n",
    "        f\"{texto}\\n\\n\"\n",
    "        f\"Por favor, redactá un resumen claro, conciso y formal en 5 a 8 líneas.\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": modelo,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Sos un asistente jurídico especializado en documentos oficiales.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.4,\n",
    "        \"max_tokens\": 500\n",
    "    }\n",
    "\n",
    "    response = requests.post(CHATGPT_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e0a841e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_nuevos_avisos.empty:\n",
    "    guardar_avisos(df_nuevos_avisos,database)\n",
    "    for index, row in df_nuevos_avisos.iterrows():\n",
    "        link_aviso = row['Link']\n",
    "        resolucion = row['Resolución']\n",
    "\n",
    "        ruta_completa = descargar_pdf(link_aviso,resolucion)\n",
    "        texto = extraer_texto_pdf(ruta_completa, guardar_txt=\"salida.txt\")\n",
    "\n",
    "        #print(texto)\n",
    "        # hf_token = \"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\" \n",
    "        # match_considerando = re.search(r'CONSIDERANDO(.*?)(RESUELVE|ARTÍCULO\\s+\\d+)', texto, re.DOTALL)\n",
    "        # match_resuelve = re.search(r'RESUELVE(.*)', texto, re.DOTALL)\n",
    "\n",
    "        # considerando = match_considerando.group(1).strip() if match_considerando else \"\"\n",
    "        # resuelve = match_resuelve.group(1).strip() if match_resuelve else \"\"\n",
    "\n",
    "        # resumen_considerando = resumir_con_gpt4(considerando, \"CONSIDERANDO\")\n",
    "        # print(resumen_considerando)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # #resumen = resumen_partes_pdf_espanol(ruta_completa, hf_token)\n",
    "        # # resumen = resumen_desde_huggingface(ruta_completa)\n",
    "        # resumen = resumen_partes_pdf_traducido(ruta_completa, hf_token)\n",
    "        # print(resumen)\n",
    "        break\n",
    "        # df_avisos.at[index, \"Resumen\"] = resumen\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
